---
title: "p8105_Hw5_jkh2157"
output: github_document
date: "2025-11-11"
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
set.seed(1)
```

## Question 1

## Creating the birthday simulation function 
```{r}
bday_sim = function(n_room) {
  birthdays = sample(1:365, n_room, replace = TRUE)
  repeated_bday = length(unique(birthdays)) < n_room
  repeated_bday
}
bday_sim(20)
```

## Running simulaton for room sizes 2-50 

```{r}
bday_sim_results = 
  expand_grid(
    bdays = 2:50,
    iter = 1:10000   
  ) %>% 
  mutate(
    results = map_lgl(bdays, bday_sim)
  ) %>% 
  group_by(bdays) %>% 
  summarize(
    prob_repeat = mean(results)
  )

#adding summary table 
bday_sim_results %>% 
  knitr::kable(
    digits = 3,
    caption = "Estimated probability that at least two people share a birthday"
  )

```

As the group sizes increase, it becomes more likely that there will be atleast 2 people who share the same birthday. 

## Creating visualization 

```{r}
bday_sim_results %>% 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Probability that at least two people share a birthday",
    x = "Group size (n)",
    y = "Probability of shared birthday"
  )
```

## Question 2 

## Creating T-Test function 
```{r}
sim_ttest = function(mu, n = 30, sigma = 5) {
  
  x = rnorm(n, mean = mu, sd = sigma)
  
  t_out = t.test(x, mu = 0)
  
  broom::tidy(t_out) %>% 
    select(estimate, p.value) %>% 
    rename(
      mu_hat  = estimate,
      p_value = p.value
    )
}
```

## Running Simulation 

```{r}
sim_results = 
  expand_grid(
    true_mu = 0:6,
    iter    = 1:5000
  ) %>% 
  mutate(
    sim = map(true_mu, sim_ttest)
  ) %>% 
  unnest(sim) %>% 
  mutate(
    reject = p_value < 0.05
  )

sim_results %>% 
  filter(true_mu %in% 0:6) %>%       
  select(true_mu, iter, mu_hat, p_value, reject) %>% 
  group_by(true_mu) %>% 
  slice_head(n = 5) %>%              
  knitr::kable(
    digits = 3,
    caption = "**Example of raw simulation results for μ = 0–6 (first 5 per group)**"
  )
```

Table only shows a few values since it was iterated 5000 times for each mu

## Summarizing power and then making a plot 

```{r}
power_results=
  sim_results %>% 
  group_by(true_mu) %>% 
  summarize(
    power=mean(reject)
  )

power_results %>% 
  ggplot(aes(x=true_mu,y=power))+
  geom_point()+
  geom_line()+
  labs(
    title="Power of one-sample t test vs true mean",
    x="True value of mu",
    y="Power (proportion of rejections)")+
  theme(
    plot.title=element_text(face="bold")
  )
```

As shown by the graph, when there is a larger effect size, there is higher power, and therefore more rejections. 

## Average mu vs true 

```{r}
mean_results = 
  sim_results %>% 
  group_by(true_mu) %>% 
  summarize(
    mean_all    = mean(mu_hat),
    mean_reject = mean(mu_hat[reject==TRUE])
  )

mean_results %>% 
  ggplot(aes(x = true_mu)) +
  geom_point(aes(y = mean_all, color = "All samples")) +
  geom_line( aes(y = mean_all, color = "All samples")) +
  geom_point(aes(y = mean_reject, color = "Rejected H0 only")) +
  geom_line( aes(y = mean_reject, color = "Rejected H0 only")) +
  labs(
    title = "Average estimate of mu across simulations",
    x = "True value of mu",
    y = "Average of mu"
  ) +
  scale_color_discrete(name = "") +
  theme(
    plot.title = element_text(face = "bold")
  )
```

As the graph shows, the average of mu across rejected tests is not equal to the true mu. This is because its biased upwards (especially when the true mu is smaller) since only the extreme (significant) samples are included. This illustrates a case of selection bias. 

## Question 3 

## Import data set 

```{r}
homicide_df = read_csv("Data/homicide-data.csv")

homicide_df = homicide_df %>%
  mutate(city_state = str_c(city, ", ", state))
```

The data set contains information on appx 55,000 homicide cases reported by 50 large US city police departments between 2007-2017. Each row represents a single homicide case, and include information about the victim, location, and case outcome. 

* uid – a unique identifier for each homicide case.
* reported_date – date the homicide was reported.
* victim_first, victim_last, victim_age, victim_sex, victim_race – demographic information about the victim.
* city and state – where the homicide occurred.
* lat and lon – latitude and longitude coordinates for the homicide location.
* disposition – case status, which can take one of three values: Closed by arrest, Closed without arrest (exceptionally cleared), Open/No arrest



